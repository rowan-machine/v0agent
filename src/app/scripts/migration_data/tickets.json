[
  {
    "ticket_id": "Dev-8109",
    "title": "[Rx Claims History] [Data ENG] Add ldd, sdl, and biosimilar to enriched claims file",
    "description": "We need to capture whether the NDC of a claim falls in the LDD, SDL, or BIOSIMILAR drug lists during enrichment.\r\n\r\nThis means accessing the rxdrug table in the q2c database and cross referencing it agains the NDC on the claims themselves.\r\n\r\nThe rxdrug table consists of the following columns: ndc, quoterequestid, quoteresponseid, and listtype.\r\n\r\nlisttype is an enum with values: GLOBAL_BIOSIMILAR, LDD, and SDL\r\n\r\n\r\n\r\nWith this ticket, during the enrichment_v1.py task we will need to add a 5th step that adds the ldd, sdl, and Biosimilar columns to the enriched rx claims file.\r\n\r\nWe will also need to add a step that loads these flags for the quoteresponseid during the reprice_v1.py task before repricing for a a quote response option.\r\n\r\nAcceptance Criteria\r\n\r\nCreate an rx_drug_repository that has the following methods:\r\n\r\nfind_rx_drug_by_quote_request_id(self, quote_request_id: int) - returns all rxdrug entries with the given quoterequestid \r\n\r\nfind_rx_drug_by_quote_response_id(self, quote_response_id: int) - returns all rxdrug entries with the given quoteresponseid\r\n\r\nfind_rx_drug_by_list_type(self, list_type: str) - returns all rxdrug entries of the given listtype. For right now this will be used to get the BIOSIMILAR type. BIOSIMILAR rxdrugs are global, so they do not have a request or response id.\r\n\r\nAdd an RxDrugBuilder class similar to the medispan builders in enrichment_v1.py that uses the new rx_drug_repository to load all the rxdrug entries for the quoterequestid we are enriching claims for.\r\n\r\nbuild should build a dataframe of unique NDCs that have a boolean flag for ldd, sdl, and Biosimilar.\r\n\r\nldd is true if the NDC appears in the listtype = LDD false otherwise\r\n\r\nsdl is true if the NDC appears in the listtype = SDL false otherwise\r\n\r\nBiosimilar is true if the NDC appears in the listtype = BIOSIMILAR false otherwise\r\n\r\nThen use the resulting dataframe to apply these columns to the enhanced claims dataframe/file\r\n\r\nmerge on NDC so that the ldd, sdl, and Biosimilar boolean flags populate\r\n\r\n\r\n\r\nReprice Changes\r\n\r\nAdd a step during the reprice_v1 dag task that does the same thing outlined above but for when we are running a quoteresponseoption reprice. This step would overwrite the flags in the enhanced file before repricing claims.\r\n\r\nThe quote request flags are only applicable to the quote request which is why we need to apply different logic for quote response options\r\n",
    "status": "complete",
    "priority": "medium",
    "sprint_points": 5,
    "in_sprint": true,
    "ai_summary": "**Summary of Jira Ticket Dev-8109:**\r\n\r\n1. **What needs to be done:** Implement a mechanism in the claims enrichment process to flag drug claims as LDD, SDL, or BIOSIMILAR based on NDCs from the rxdrug table. This includes extending the enrichment and repricing tasks to incorporate these flags.\r\n\r\n2. **Key technical details:** \r\n   - Access the `rxdrug` table in the `q2c` database, focusing on columns: `ndc`, `quoterequestid`, `quoteresponseid`, and `listtype` (with enum values: `GLOBAL_BIOSIMILAR`, `LDD`, and `SDL`).\r\n   - Create an `rx_drug_repository` with methods to query by `quote_request_id` and `quote_response_id`.\r\n   - Develop an `RxDrugBuilder` class to construct a DataFrame with unique NDCs and corresponding boolean flags for LDD, SDL, and BIOSIMILAR.\r\n   - The enrichment step must merge this DataFrame with the enriched claims file. A parallel step in the `reprice_v1.py` task is needed to overwrite flags specific to quote responses.\r\n\r\n3. **Dependencies or blockers:** None mentioned in the ticket; however, the implementation relies on access to the `q2c` database and completion of dependencies related to the enrichment tasks and their orchestration in the existing environment.",
    "implementation_plan": "None",
    "task_decomposition": [
      {
        "text": "Create the rx_drug_repository with methods: find_rx_drug_by_quote_request_id, find_rx_drug_by_quote_response_id, and find_rx_drug_by_list_type",
        "estimate": "3h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Develop the RxDrugBuilder class to load rxdrug entries and build the required DataFrame with boolean flags",
        "estimate": "4h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Implement the logic to merge the DataFrame with the enhanced claims DataFrame in enrichment_v1.py",
        "estimate": "2h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Add the new step in reprice_v1.py to apply and overwrite flags for quote response options",
        "estimate": "2h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Write unit tests for the rx_drug_repository methods",
        "estimate": "2h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Write unit tests for the RxDrugBuilder class functionality",
        "estimate": "2h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Test the integration of new flags in the entire claims enrichment pipeline",
        "estimate": "3h",
        "done": true,
        "status": "done"
      },
      {
        "text": "Document the new functionalities and changes made in the codebase",
        "estimate": "1h",
        "done": true,
        "status": "done"
      },
      {
        "text": "the drug lists are at the quote response level not the option",
        "done": false
      }
    ],
    "tags": [
      "airflow",
      "pipeline"
    ]
  },
  {
    "ticket_id": "SIG-2",
    "title": "Establish a standard \u201cworkflow + validator\u201d checklist.",
    "description": "Signal from: DMP<>CEO\r\n\r\nOriginal Signal (idea):\r\nEstablish a standard \u201cworkflow + validator\u201d checklist.",
    "status": "todo",
    "priority": "low",
    "sprint_points": 0,
    "in_sprint": false,
    "ai_summary": "Converted from idea signal: Establish a standard \u201cworkflow + validator\u201d checklist....",
    "implementation_plan": "None",
    "task_decomposition": null,
    "tags": [
      "None"
    ]
  },
  {
    "ticket_id": "AI-3",
    "title": "DMP Workflow Validation**: Ensure that validation patterns are defined as part of workflow impleme",
    "description": "From AI Query: reach_outs\r\n\r\nAI Response:\r\n**DMP Workflow Validation**: Ensure that validation patterns are defined as part of workflow implementation and outline system and product KPIs for dashboards.",
    "status": "todo",
    "priority": "medium",
    "sprint_points": 0,
    "in_sprint": false,
    "ai_summary": "AI insight: **DMP Workflow Validation**: Ensure that validation patterns are defined as part of workflow implementation and outline system and product KPIs for da...",
    "implementation_plan": "None",
    "task_decomposition": null,
    "tags": [
      "None"
    ]
  },
  {
    "ticket_id": "Dev-8142",
    "title": "[Rx Audit Claims] [Data ENG] Migrate Rx Audit Claims pipeline to dmpv2 with unified load DAG and PBM config resolution",
    "description": "User Story\r\n\r\nAs a data engineer / platform engineer, I want to refactor the existing Rx Audit Claims pipelines into a single dmpv2-native load pipeline so that Rx Audit ingestion, validation, and upload behavior is easier to maintain, consistent with other dmpv2 domains, and safer to extend as new PBMs or partners are added.\r\n\r\nNotes on dmpv1 pipelines:\r\n\r\nThe current dmpv1.rx_audit implementation is scattered across multiple pipelines (process_rx_audit_claims_dag, post_rx_audit_claims_dag, rx_audit_claims_load_dag, and their _all variants), but the underlying behavior is identical (differences are driven entirely by how inputs are resolved). The core processing logic lives in dmpv1/plugins/rmtdmp/process_rx_audit_claims.py (the _rx_audit_claims_process class) and dmpv1/plugins/rmtdmp/post_rx_audit_claims.py (the _rx_audit_claim_uploader class). These classes handle PBM-specific transformations by loading multiple JSON configuration files from config/definitions/ and applying nested lookups based on claim_type \u2192 partner \u2192 provider.\r\n\r\nThe existing config structure uses 8 JSON files to define PBM-specific behavior:\r\n\r\nrx_audit_claims_columns_alias.json - Maps vendor column names to canonical names (e.g., MBR_KEY \u2192 MEMBER_ID for Anthem)\r\n\r\nrx_audit_claims_mapping_dictionary.json - Transforms values per PBM (e.g., \"MO\" \u2192 \"MAIL\" for channel indicator)\r\n\r\nrx_audit_claims_scaling_columns.json - Defines numeric scaling rules (divide by 100, 1000, 100000) for PBMs like rxbenefits/cvs\r\n\r\nrx_audit_claims_calculated_columns.json - Defines derived column formulas using sum and concat operations\r\n\r\nrx_audit_claims_default_values.json -Specifies default values for empty fields\r\n\r\nrx_audit_claims_filter_records.json - Defines record filtering conditions\r\n\r\nrx_audit_claims_overpunch.json - Contains overpunch decoding maps for mainframe file formats\r\n\r\ntxt_file_delimiters.json - Specifies file delimiters per PBM\r\n\r\nCurrently, each class independently loads these configs and performs nested dictionary lookups, which creates risk of inconsistent config resolution and makes it difficult to validate that a PBM is fully defined across all required config sections. \r\n\r\nThe processing flow involves: \r\n\r\nloading the raw file \u2192 applying overpunch decoding (if applicable) \u2192 applying column aliases \u2192 normalizing column names \u2192 applying scaling \u2192 parsing dates \u2192 applying value mappings \u2192 calculating derived columns \u2192 applying defaults \u2192 filtering records \u2192 validating via QuoteLinq API \u2192 batching into 50k-row CSVs \u2192 uploading to the QuoteLinq API.\r\n\r\nNotes on dmpv2 design:\r\n\r\nIn dmpv2, this work consolidates into a single pipeline named rxauditclaims_load_v1. The key architectural change is introducing PBMConfigRegistry; a singleton that loads all config files once, builds PBMIngestConfig dataclass instances for each (partner, provider) combination, and provides a resolve_from_path(file_key) method that parses S3 paths like signal/rxauditclaims/anthem/20250115/claims.parquet and returns the fully-resolved config. This centralizes config access and enables fail-fast validation when a PBM is missing required config entries.\r\n\r\nFile discovery is implemented as a dedicated BaseTransform (RxAuditClaimsScanV1) rather than inline DAG logic. The scan transform accepts either an explicit list of config files or a folder path, discovers eligible files via shared S3 utilities, and writes a scheduler config artifact consumed by downstream ingest steps. This follows established dmpv2 patterns used in Medispan pipelines and improves separation of concerns and testability.\r\n\r\nSingle DAG Design: This implementation uses one DAG (rxauditclaims_load_v1) rather than separate ingest and upload DAGs. Unlike the rxclaims pipeline where intermediate outputs (unenriched, enriched parquet files) have standalone business value for analysis and repricing, the rx_audit_claims intermediate output (staged CSVs) exists solely to feed the QuoteLinq API upload. The CSVs are staging artifacts with no independent use case. Separating ingest from upload would create confusion about which DAG to use and add maintenance burden without benefit. If upload fails, the pipeline should retry with exponential backoff and alert on persistent failures, not require manual re-triggering of a separate DAG. For edge cases like local debugging, the DAG accepts an optional skip_upload parameter.\r\n\r\nThe transform layer is implemented as PBMIngestTransformV1, which extends BaseTransform and delegates PBM-specific data manipulation to dedicated builder-style classes, following existing dmpv2 patters used in the enrichment domain.\r\n\r\nArchitecture Notes: Transform Utilities Pattern\r\nFollowing established dmpv2 patterns (for example, MedispanNDCBuilder and DerivedColumnsCalculator in the enrichment domain), PBM-specific transformation logic is encapsulated in builder classes rather than standalone functions in shared transform utilities. This keeps Rx Audit\u2013specific logic contained within the rxauditclaims package and avoids expanding shared utilities with domain-specific behavior.\r\n\r\nTwo builder classes are introduced:\r\n\r\nPBMDataProcessor - Handles Rx Audit data transformations including overpunch decoding, column aliasing, numeric scaling, value mappings, calculated columns, default values, and record filtering, all driven by PBMIngestConfig.\r\n\r\nCSVBatchWriter - Handles normalization to the canonical 37-column schema, output formatting for API consumption, and splitting output into 50k-row CSV batches.\r\n\r\nThe upload logic is implemented as PBMUploadTransformV1, which handles CSV batching (50k rows max), API uploads via QuoteLinqOrgService, job polling, and reversal flag determination.\r\n\r\nThe canonical output schema consists of 37 columns that all PBM files are normalized to:\r\n\r\nIGNORE, GROUP_ID, CLAIM_ID, MEMBER_ID, NDC, FILL_DATE, QUANTITY_FILLED, DAYS_SUPPLY, INGREDIENT_COST, CHANNEL_INDICATOR, PLAN_PAID, MEMBER_PAID, UC_AMOUNT, DISPENSING_FEE, NABP, NCPDP_ID, NPI, DAW, COMPOUND_INDICATOR, MAINTENANCE_INDICATOR, SPECIALTY_INDICATOR, LDD_INDICATOR, COB_INDICATOR, INDICATOR_340B, DMR_INDICATOR, FORMULARY_INDICATOR, SALES_TAX, GROSS_COST, AWP, PRICING_BASIS, NDA_ANDA, DRUG_TYPE, PLAN_ID, PATIENT_DOB, PATIENT_ZIP, PATIENT_GENDER, PBM.\r\n\r\nThis approach assumes that the current multi-config pattern is sufficient to support partner-specific behavior. Adding a new PBM requires only updating the JSON config files; no code changes or DAG deployments are needed.\r\n\r\nThis design intentionally avoids adding Rx Audit\u2013specific logic to shared transform utilities, preserving shared modules for cross-domain concerns and keeping Rx Audit behavior self-contained.\r\n\r\nAcceptance Criteria\r\n\r\nPipeline & DAG\r\n\r\nA single dmpv2 DAG named rxauditclaims_load_v1 exists at dags/domains/rx/rxauditclaims/rxauditclaims_load_v1.py\r\n\r\nThe DAG orchestrates scan \u2192 ingest \u2192 upload steps in a single flow, where file discovery is handled by a dedicated scan transform and downstream tasks consume its output.\r\n\r\nThe DAG accepts either an explicit list of config files or a folder_path parameter, which are resolved by the scan transform into a concrete file list for downstream ingestion.\r\n\r\nThe DAG accepts optional skip_upload: true parameter for testing/debugging only\r\n\r\nThe upload step includes retry logic with exponential backoff\r\n\r\nThe DAG uses RMTDMPDag base class with domain=\"rx\", workflow=\"rxauditclaims\"\r\n\r\nConfig Resolution\r\n\r\nPBMIngestConfig dataclass implemented with all PBM-specific settings (partner, provider, file_format, delimiter, column_alias, value_mappings, scaling_rules, calculated_columns, default_values, filter_conditions, overpunch config)\r\n\r\nPBMConfigRegistry singleton implemented with resolve_from_path(), get_config(), and list_configs() methods\r\n\r\nRegistry loads all 8 JSON config files from S3, caches them, and builds config instances for each (partner, provider) combination\r\n\r\nConfig precedence rules (claim_type \u2192 partner \u2192 provider) applied consistently\r\n\r\nPath parsing handles both flat structures (anthem, uhc) and nested structures (rxbenefits/cvs, rxbenefits/optum)\r\n\r\nMissing or partially defined PBM configs fail fast with clear error messages\r\n\r\nTransforms\r\n\r\nPBMIngestTransformV1 implemented extending BaseTransform and orchestrating builder classes for Rx Audit-specific data processing:\r\n\r\nFile loading based on format config (parquet, CSV, fixed-width)\r\n\r\nDelegates PBM-specific transformation steps (overpunch decoding, aliasing, scaling, value mappings, calculated columns, defaults, filtering) to PBMDataProcessor, driven entirely by PBMIngestConfig.\r\n\r\nOutput to canonical 37-column schema\r\n\r\nPBMUploadTransformV1 implemented with CSV batching (50k rows), API upload, job polling, reversal flag determination, and error handling\r\n\r\nQuoteLinqOrgService implements IOrgService for group ID validation, and a separate IUploadApiClient Protocol is introduced to abstract QuoteLinq upload operations (upload, polling, cancellation). This enables proper dependency injection and test isolation for upload behavior.\r\n\r\nPBMUploadTransformV1 delegates API communication to an injected upload client implementing IUploadApiClient (production: QuoteLinqUploadClient). This avoids direct request patching in tests and aligns with Protocol-based patterns used elsewhere in dmpv2.\r\n\r\nUpload logic validates required QuoteLinq environment configuration (base URI and API key secret name) at startup, logs explicit configuration errors, and fails fast before any API interaction if configuration is missing.\r\n\r\nBehavior is functionally equivalent to existing dmpv1 pipelines\r\n\r\nShared Utilities\r\n\r\n No Rx Audit\u2013specific transformation logic is added to shared utilities. All PBM-specific behavior is encapsulated in builder classes within the rxauditclaims package, following the same pattern used by enrichment builders (e.g., MedispanNDCBuilder). Shared utilities remain limited to cross-domain helpers only.\r\n\r\nConfiguration & Registration\r\n\r\ntransform_registry.json updated with rx.rxauditclaims.ingest and rx.rxauditclaims.upload\r\n\r\nenv_config.py updated with data_rxauditclaims_path property, following the existing naming convention used for other domain output paths.\r\n\r\nAll __init__.py files updated with proper exports\r\n\r\ndag_registry.jsonupdated with rxauditclaims_load_v1 dag configuration\r\n\r\nTesting & Validation\r\n\r\nUnit tests for PBMDataProcessor and CSVBatchWriter covering transformation behavior and output batching.\r\n\r\nUnit tests for PBMIngestConfig covering dataclass fields, defaults, and property behavior\r\n\r\nUnit tests for PBMConfigRegistry covering path parsing, config resolution, nested vs flat lookups, and error handling\r\n\r\nUnit tests for PBMIngestTransformV1 covering each transformation method independently\r\n\r\nUnit tests for PBMUploadTransformV1 covering CSV batching, reversal flag logic, and API interaction mocking\r\n\r\nUnit tests for QuoteLinqOrgService covering API calls, error handling, and response parsing\r\n\r\nIntegration tests validating end-to-end flow for at least one PBM (anthem), plus contract-level integration tests asserting config completeness and schema correctness across all supported PBMs.\r\n\r\nPipeline runs end-to-end without regressions compared to dmpv1\r\n\r\nConfig resolution validated for all 7 supported PBM combinations\r\n\r\nStretch Goals\r\n\r\nContract-level PBMConfigValidator that performs cross-file validation for all PBMs, including required config presence, structural correctness, and partner-type\u2013specific rules (standard vs. mainframe).\r\n\r\nA CLI developer utility (scripts/validate_pbm_configs.py) supports validation, inspection, CI/CD enforcement, and partner onboarding workflows.\r\n\r\n",
    "status": "todo",
    "priority": "medium",
    "sprint_points": 0,
    "in_sprint": false,
    "ai_summary": "- **Goal:** Refactor the existing Rx Audit Claims pipelines into a single dmpv2-native load pipeline, named `rxauditclaims_load_v1`, to enhance maintainability, consistency, and extensibility for new PBMs or partners.\r\n\r\n- **Key Details:** The new pipeline will support both single-file and load-all executions by using runtime parameters. Implementation will leverage structured Python classes instead of DAG-level logic, with centralized configuration resolution through `RxAuditPBMConfig` and `PBMConfigRegistry`. The refactor aims to ensure that PBM configurations are consistent and errors are handled clearly. Testing will include contract tests for PBM configurations and unit tests for pipeline behavior.\r\n\r\n- **Dependencies:** Completion of this ticket requires existing Rx Audit configurations to be structured adequately for the new config loader. There may also be a need to revisit the config structure if partner onboarding becomes more complex in the future.\r\n\r\n- **Complexity:** Medium - The refactor involves significant changes in architecture (from multiple pipelines to a single one) and requires careful handling of configuration and validation processes, but existing structures will be preserved.",
    "implementation_plan": "## Sprint Implementation Plan for Ticket: Dev-8142 - Rx Audit Claims Pipeline Migration\r\n\r\n### 1. Approach\r\nThe objective is to refactor the existing Rx Audit Claims pipelines into a single dmpv2-native load pipeline, `rxauditclaims_load_v1`, improving maintainability and consistency by using structured Python classes for configuration resolution. We will centralize the configuration loading process while ensuring backward compatibility with existing behavior, followed by rigorous testing to ensure that functionality remains intact.\r\n\r\n### 2. Steps\r\n1. **Set Up Development Environment**\r\n   - Ensure all necessary dependencies for Airflow, Python, and AWS are installed and configured properly.\r\n   - Clone the repository and create a feature branch from the current main branch.\r\n\r\n2. **Analyze Current dmpv1 Implementation**\r\n   - Review the existing dmpv1 Rx Audit pipelines in the repository to understand the current configuration and workflow.\r\n   - Identify variations across the multiple pipelines related to input resolution.\r\n\r\n3. **Design New Pipeline Structure**\r\n   - Sketch out the new architecture for `rxauditclaims_load_v1`, highlighting the use of runtime parameters for execution modes (single-file vs. load-all).\r\n   - Plan the structure for new Python classes: `RxAuditPBMConfig`, `PBMConfigRegistry`, `PBMIngestTransformV1`, and `RxAuditClaimsUploadV1`.\r\n\r\n4. **Implement Configuration Classes**\r\n   - Create the `RxAuditPBMConfig` class to load PBM definition files based on parameters.\r\n   - Develop `PBMConfigRegistry` to store and manage configurations for quick access.\r\n\r\n5. **Develop the DAG**\r\n   - Create `rxauditclaims_load_v1` DAG in the appropriate directory.\r\n   - Implement tasks for combine, ingest, and upload steps using dmpv2 patterns.\r\n\r\n6. **Implement Transforms**\r\n   - Code the `PBMIngestTransformV1` class for ingestion and validation processing.\r\n   - Create `RxAuditClaimsUploadV1` for handling the API uploads, error handling, and job polling.\r\n\r\n7. **Testing and Validation**\r\n   - Write contract tests for PBM configuration completeness and integration tests for pipeline execution.\r\n   - Run unit tests to validate the correctness of the ingestion and upload processes.\r\n\r\n8. **Documentation**\r\n   - Update existing documentation to reflect changes in configuration structure and usage of the new pipeline.\r\n   - Document the new implementation details and usage of runtime parameters.\r\n\r\n9. **Code Review and Merge**\r\n   - Submit a merge request for review, addressing any feedback and ensuring all tests pass.\r\n   - Merge the feature branch into the main branch once all checks are complete.\r\n\r\n### 3. Files to Modify\r\n- **DAG File**: Create `dags/rxauditclaims_load_v1.py` for the main pipeline.\r\n- **Configuration Directory**: Modify or implement classes in `src/config/definitions` for `RxAuditPBMConfig` and `PBMConfigRegistry`.\r\n- **Transform Modules**: Create or modify transformation classes in `src/transforms/`.\r\n- **Tests Directory**: Add new test cases in `tests/test_rxauditclaims.py` for pipeline and configuration testing.\r\n- **Documentation**: Update `docs/architecture.md` or equivalent for new design details.\r\n\r\n### 4. Testing\r\n- **Unit Tests**: Create unit tests for each new class to ensure correct behavior.\r\n- **Integration Tests**: Validate end-to-end functionality by executing the pipeline and checking outputs for at least one existing PBM.\r\n- **Contract Tests**: Ensure relevant PBM configurations are fully defined and adhere to expected structure.\r\n- **Validation Utility**: Optionally develop and test a lightweight utility for inspecting resolved PBM configurations.\r\n\r\n### 5. Risks\r\n- **Configuration Compatibility**: If existing Rx Audit configurations are not structured correctly, this may lead to errors during pipeline execution.\r\n- **Backward Compatibility**: Changes may inadvertently affect the behavior of existing integrations if not carefully managed.\r\n- **Testing Coverage**: Insufficient testing may lead to missed regressions or issues in the pipeline's behavior.\r\n- **Deployment Timing**: Ensure this implementation aligns with upcoming partner onboarding to avoid delays.\r\n\r\nThis implementation plan aims to provide a structured and actionable approach to migrating the Rx Audit Claims pipeline, ensuring robust delivery against the established acceptance criteria while mitigating risks.",
    "task_decomposition": [
      {
        "text": "Implement PBMIngestConfig dataclass, PBMConfigRegistry, and PBMConfigValidator",
        "done": false
      },
      {
        "text": "Update configuration and registration files for Rx audit pipeline",
        "done": false
      },
      {
        "text": "Implement PBMUploadTransformV1 with Protocol-based upload client",
        "done": false
      },
      {
        "text": "Implement PBMDataProcessor and PBMIngestTransformV1",
        "done": false
      },
      {
        "text": "Refactor Rx Audit transform logic into builder classes (PBMDataProcessor, CSVBatchWriter)",
        "done": false
      },
      {
        "text": "Implement RxAuditClaimsScanV1 transforms for file discovery",
        "done": false
      },
      {
        "text": "Implement QuoteLinqOrgService, IOrgService, IUploadApiClient, and QuoteLinqUploadClient",
        "done": false
      },
      {
        "text": "Develop rxauditclaims_load_v1 DAG with scan \u2192 ingest \u2192 upload flow",
        "done": false
      },
      {
        "text": "Write unit tests for config, processors, and transforms.",
        "done": false
      },
      {
        "text": "Develop integration tests for end-to-end flow",
        "done": false
      },
      {
        "text": "Add CLI tool for PBM config validation and inspection",
        "done": false
      },
      {
        "text": "Add contract-level integration tests for PBM config completeness",
        "done": false
      },
      {
        "text": "Add developer documentation for Rx Audit PBM config system",
        "done": false
      }
    ],
    "tags": [
      "airflow",
      "pipeline",
      "claims",
      "pbm"
    ]
  },
  {
    "ticket_id": "AI-5",
    "title": "Write details in the ticket for the RX audit claims refactor.",
    "description": "From AI Query: rowan_mentions\r\n\r\nAI Response:\r\nWrite details in the ticket for the RX audit claims refactor.",
    "status": "todo",
    "priority": "medium",
    "sprint_points": 0,
    "in_sprint": false,
    "ai_summary": "AI insight: Write details in the ticket for the RX audit claims refactor....",
    "implementation_plan": "None",
    "task_decomposition": null,
    "tags": [
      "None"
    ]
  },
  {
    "ticket_id": "CAREER-9",
    "title": "Develop a Data Lineage and Catalog Tool",
    "description": "Lead a project within your current role to create or enhance a data lineage and cataloging tool. Focus on integrating features that support knowledge management practices and leverage existing enterprise solutions.\r\n\r\n**Rationale:** This projects will utilize your strengths in data architecture and enterprise solutions while addressing your interests in data lineage and management, showcasing your ability to lead and innovate.\r\n**Difficulty:** advanced\r\n**Time:** 3-6 months",
    "status": "todo",
    "priority": "medium",
    "sprint_points": 0,
    "in_sprint": false,
    "ai_summary": "None",
    "implementation_plan": "None",
    "task_decomposition": null,
    "tags": [
      "career",
      "growth"
    ]
  }
]