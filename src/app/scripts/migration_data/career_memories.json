[
  {
    "memory_type": "completed_project",
    "title": "[Rx Claims History] [Data ENG] Add ldd, sdl, and biosimilar to enriched claims file",
    "description": "**Summary of Jira Ticket Dev-8109:**\r\n\r\n1. **What needs to be done:** Implement a mechanism in the claims enrichment process to flag drug claims as LDD, SDL, or BIOSIMILAR based on NDCs from the rxdrug table. This includes extending the enrichment and repricing tasks to incorporate these flags.\r\n\r\n2. **Key technical details:** \r\n   - Access the `rxdrug` table in the `q2c` database, focusing on columns: `ndc`, `quoterequestid`, `quoteresponseid`, and `listtype` (with enum values: `GLOBAL_BIOSIMILAR`, `LDD`, and `SDL`).\r\n   - Create an `rx_drug_repository` with methods to query by `quote_request_id` and `quote_response_id`.\r\n   - Develop an `RxDrugBuilder` class to construct a DataFrame with unique NDCs and corresponding boolean flags for LDD, SDL, and BIOSIMILAR.\r\n   - The enrichment step must merge this DataFrame with the enriched claims file. A parallel step in the `reprice_v1.py` task is needed to overwrite flags specific to quote responses.\r\n\r\n3. **Dependencies or blockers:** None mentioned in the ticket; however, the implementation relies on access to the `q2c` database and completion of dependencies related to the enrichment tasks and their orchestration in the existing environment.",
    "skills": [
      "airflow",
      "pipeline"
    ],
    "source_type": "ticket",
    "is_pinned": false,
    "is_ai_work": false
  },
  {
    "memory_type": "ai_implementation",
    "title": "Async LLM Query with Caching",
    "description": "This pattern implements asynchronous calls to the language model (LLM) with caching results to minimize redundant requests and improve performance. The pattern can be observed in `src/app/llm.py` where the `ask` function retrieves results from the LLM and can be cached based on input prompts.\n\n**Code Insight:** The function `ask` in the LLM integrates asynchronous API calls, while the inclusion of a caching mechanism (not explicitly shown, but can be implemented) would prevent duplicate queries by storing responses keyed by the prompts.",
    "skills": [
      "FastAPI",
      "OpenAI",
      "SQLite"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Multi-Provider Model Routing",
    "description": "The codebase is designed to interact with multiple language model providers (e.g., OpenAI and Anthropic). It selects the appropriate model based on configuration or fallback options in `src/app/llm_new.py`. This allows for flexibility and optimizes model usage based on request type.\n\n**Code Insight:** Implementing model routing via the method `_openai_client_once` and `_anthropic_client_once` that manages client instantiation ensures that the application can readily switch between different AI models depending on specified settings.",
    "skills": [
      "OpenAI",
      "Anthropic"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Error Handling with Graceful Degradation",
    "description": "The system includes error handling protocols that allow the application to fail gracefully, particularly seen in the handling of Neo4j synchronization failures in `src/app/meetings.py` and `src/app/documents.py`. If Neo4j is unavailable, the system skips the synchronization without crashing.\n\n**Code Insight:** Both `process_screenshots` and document storage functions include try-except blocks that log errors while allowing the function to continue its execution.",
    "skills": [
      "FastAPI",
      "SQLite",
      "optional external services"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "State Management with In-Memory Session Tokens",
    "description": "The implementation manages user sessions through in-memory tokens stored in the `_sessions` dictionary within `src/app/auth.py`. Each session token has an associated metadata dictionary that tracks user activity.\n\n**Code Insight:** The use of an in-memory dictionary for session management as seen in `create_session` and `validate_session` functions is straightforward but can lead to scalability issues; it's recommended to transpire to a more persistent storage solution like Redis for production use.",
    "skills": [
      "FastAPI",
      "Python standard libraries"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Prompt Engineering through Dynamic Configuration",
    "description": "The codebase utilizes environment variables and stored settings in the database to dynamically configure LLM prompts, especially in `src/app/llm.py`. The `SYSTEM_PROMPT` configuration adjusts the LLM's instructions at runtime based on context.\n\n**Code Insight:** The system prompts are configured to guide how the LLM responds, enhancing the contextual relevance of the results, as showcased by the `SYSTEM_PROMPT` variable which is crucial for shaping the LLM\u2019s responses.",
    "skills": [
      "OpenAI",
      "FastAPI",
      "SQLite"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Async LLM Streaming with Fallback",
    "description": "The system uses FastAPI for asynchronous API endpoints that interact with LLMs. In scenarios where LLM requests take time, responses fallback to pre-cached or stored data while awaiting LLM output, ensuring responsive UI while processing heavy queries.\n\n**Code Insight:** The `run_turn` function summarizes the queries before passing them to the LLM, allowing the system to first try fetching any pre-stored answers if they exist.",
    "skills": [
      "FastAPI",
      "OpenAI API",
      "SQLAlchemy"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Model and API Layer Abstraction",
    "description": "Multiple model providers are integrated where the application can select different AI models based on configuration or runtime parameters. This approach allows for flexible and efficient switching between models like OpenAI's GPT and Anthropic's Claude with minimal changes to the codebase.\n\n**Code Insight:** The implementation of `_openai_client_once` and `_anthropic_client_once` in `llm_new.py` ensures that APIs for different providers are initialized only once, which conserves resources during repeated calls.",
    "skills": [
      "OpenAI Python client",
      "Anthropic client"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "In-Memory and Persistent Caching",
    "description": "The codebase implements caching methodologies for storing session data in memory while also persisting to a database. This dual approach minimizes latency by accessing in-memory sessions for the active user while ensuring that sessions remain persistent after server restarts.\n\n**Code Insight:** The `_sessions` dictionary in `auth.py` provides quick access to user sessions while the session details are also saved in SQLite, allowing the system to recover sessions during a restart.",
    "skills": [
      "SQLite",
      "Python dictionaries"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Error Handling and Resilience",
    "description": "Throughout the codebase, error handling is implemented to ensure that the application remains operational even if certain services fail. Fallbacks or silent failures (e.g., optional Neo4j sync or API calls) are judiciously applied, ensuring that users are not impacted by non-critical errors.\n\n**Code Insight:** Usage of `try-except` blocks, such as in the handling of the Neo4j sync, demonstrates the code's design to tolerate failures gracefully, maintaining functionality without compromising user experience.",
    "skills": [
      "FastAPI",
      "SQLite"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  },
  {
    "memory_type": "ai_implementation",
    "title": "Dynamic Prompt Engineering for LLMs",
    "description": "The application dynamically constructs prompts for LLM queries incorporating contextual information from user inputs and predefined contexts, optimizing engagement and response quality. These prompts are adjustable based on operational needs or user inputs.\n\n**Code Insight:** In `ask` function in `llm.py`, the prompt is constructed from user inputs before sending it to the LLM, especially emphasizing context for better performance in response generation.",
    "skills": [
      "OpenAI API",
      "string formatting"
    ],
    "source_type": "codebase_ai",
    "is_pinned": false,
    "is_ai_work": true
  }
]